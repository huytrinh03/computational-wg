# computational-wg

This repository contains the procedures for different processes executed by the Computational Work Group:
- Data collection: this process includes web scraping and data cleaning
- CV creation: creating CVs from the collected employer data and fake applicant information

## 1. Data collection: This process is different across differen job boards in India, as each site has a different webpage logic and thus requires a different approach to scrape and clean data from it. However, there are 2 common steps for every site:
- Scrape using the Web Scraper Chrome extension
- Parse and clean the data using Python
### 1.1. Naukri
#### 1.1.1. Web scraping
1. Copy the content inside the sitemap.json file
2. 
### 1.2. Foundit
### 1.3. Shine

Related documentation:
Web Scraper Chrome extension
![image](https://github.com/user-attachments/assets/1ec9ea83-08af-4704-91d2-ca490cd5a260)

